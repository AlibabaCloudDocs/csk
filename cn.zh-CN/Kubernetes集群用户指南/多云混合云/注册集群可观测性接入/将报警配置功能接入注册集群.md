---
keyword: [报警配置, 报警规则模板, 报警, 监控]
---

# 将报警配置功能接入注册集群

报警配置功能提供容器场景报警能力的统一管理，包括容器服务异常事件报警，以及容器场景指标报警。您可以通过集群内部署CRD的方式配置容器服务的报警规则。本文介绍在注册集群中如何接入报警配置功能及如何配置报警规则。

-   您需要创建一个注册集群，并将自建Kubernetes集群接入注册集群。具体操作，请参见[创建注册集群并接入本地数据中心集群](/cn.zh-CN/Kubernetes集群用户指南/多云混合云/注册集群管理/创建注册集群并接入本地数据中心集群.md)。
-   通过kubectl连接注册集群。具体操作，请参见[通过kubectl管理Kubernetes集群](/cn.zh-CN/Kubernetes集群用户指南/集群/连接集群/通过kubectl管理Kubernetes集群.md)。

## 功能使用场景

容器服务报警配置功能集合容器场景的监控报警能力，提供报警的统一配置管理，有如下几个典型的使用场景：

-   **集群运维**

    可以通过监控报警第一时间了解集群管控、存储、网络、弹性扩缩容等异常事件。例如，可通过配置并查看**集群异常事件报警规则集**感知集群节点或容器节点通用异常；通过配置并查看**集群存储异常事件报警规则集**感知集群存储的变更与异常；通过配置并查看**集群网络异常事件报警规则集**感知集群网络的变更与异常；通过配置并查看**集群管控运维异常报警规则集**感知集群管控的变更与异常等。

-   **应用开发**

    可以通过监控报警第一时间了解在集群中运行应用的异常事件、指标是否异常。例如，集群容器副本异常或者应用Deployment的CPU、内存水位指标是否超过阈值等。可通过开启报警配置功能中的默认报警规则模板，即可快速接受集群内应用容器副本的异常事件报警通知。例如，通过配置并订阅关注**集群容器副本异常报警规则集**感知所属应用的Pod是否异常。

-   **应用管理**

    关注运行在集群上的应用健康、容量规划、集群运行稳定性及异常甚至是错误报警等贯穿应用生命周期的一系列问题。例如，通过配置并订阅关注**集群重要事件报警规则集**感知集群内所有Warning、Error等异常报警；关注**集群资源异常报警规则集**感知集群的资源情况，从而更好地做容量规划等。

-   **多集群管理**

    当您有多个集群需要管理，为集群配置报警规则往往会是一个重复繁琐且难以同步的操作。容器服务报警配置功能，支持通过集群内部署**CRD配置的方式管理报警规则**。可通过维护多个集群中同样配置的CRD资源，来方便快捷地实现多集群中报警规则的同步配置。


## 在注册集群中配置云监控组件

**步骤一：为云监控组件配置RAM权限**

在注册集群中安装组件前，您需要在接入集群中设置AccessKey用来访问云服务的权限。设置AccessKey前，您需要创建RAM用户并为其添加访问相关云资源的权限。

1.  创建RAM用户。具体操作，请参见[创建RAM用户](/cn.zh-CN/用户管理/基本操作/创建RAM用户.md)。

2.  创建权限策略。具体操作，请参见[创建自定义策略](/cn.zh-CN/权限策略管理/自定义策略/创建自定义策略.md)。

    权限策略模板如下所示：

    ```
    {
                "Action": [
                    "log:*",
                    "arms:*",
                    "cs:UpdateContactGroup"
                ],
                "Resource": [
                    "*"
                ],
                "Effect": "Allow"
    }
    ```

3.  为RAM用户添加权限。具体操作，请参见[为RAM用户授权](/cn.zh-CN/用户管理/授权管理/为RAM用户授权.md)。

4.  为RAM用户创建AccessKey。具体操作，请参见[获取AccessKey]()。

5.  使用AccessKey在注册集群中创建名为alibaba-addon-secret的Secret资源。

    安装云监控组件时将自动引用此AccessKey访问对应的云服务资源。

    ```
    kubectl -n kube-system create secret generic alibaba-addon-secret --from-literal='access-key-id=<your access key id>' --from-literal='access-key-secret=<your access key secret>'
    ```

    **说明：** `<your access key id>`及`<your access key secret>`为上一步获取的AccessKey信息。


**步骤二：安装与升级云监控组件**

控制台会自动检测报警配置环境是否符合要求，并会引导进行开通、安装或升级组件。

1.  登录[容器服务管理控制台](https://cs.console.aliyun.com)。

2.  在控制台左侧导航栏中，单击**集群**。

3.  在集群列表页面中，单击目标集群名称或者目标集群右侧**操作**列下的**详情**。

4.  在集群管理页左侧导航栏，选择**运维管理** \> **报警配置**。

5.  在报警配置页面控制台会自动检查以下条件。

    若不符合条件，请按以下提示完成操作。

    -   已开通SLS日志服务云产品。当您首次使用日志服务时，需要登录[日志服务控制台](https://sls.console.aliyun.com)，根据页面提示开通日志服务。
    -   已安装**事件中心**。具体操作，请参见[事件监控](/cn.zh-CN/Kubernetes集群用户指南/可观测性/监控管理/事件监控.md)。
    -   集群托管组件alicloud-monitor-controller升级到最新版本。更多信息，请参见[alicloud-monitor-controller](/cn.zh-CN/产品发布记录/组件介绍与变更记录/日志与监控/alicloud-monitor-controller.md)。
    ![报警配置](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6282476161/p255151.png)


## 接入报警配置功能

**步骤一：开启默认报警规则**

1.  登录[容器服务管理控制台](https://cs.console.aliyun.com)。

2.  在控制台左侧导航栏中，单击**集群**。

3.  在集群列表页面中，单击目标集群名称或者目标集群右侧**操作**列下的**详情**。

4.  在集群管理页左侧导航栏，选择**运维管理** \> **报警配置**。

5.  在报警规则管理页签，开启对应报警规则集。

    ![报警规则管理](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/9749131261/p249722.png)

    具体操作，请参见[步骤二：手动配置报警规则](#step_pjq_wu5_deg)。


**步骤二：手动配置报警规则**

1.  登录[容器服务管理控制台](https://cs.console.aliyun.com)。

2.  在控制台左侧导航栏中，单击**集群**。

3.  在集群列表页面中，单击目标集群名称或者目标集群右侧**操作**列下的**详情**。

4.  在集群管理页左侧导航栏，选择**运维管理** \> **报警配置**。

    |功能特性|说明|
    |----|--|
    |**报警规则管理**|    -   容器服务报警规则功能会默认生成容器场景下的报警模板（包含异常事件报警、异常指标报警）。
    -   报警规则被分类为若干个报警规则集，可为报警规则集关联多个联系人分组，并启动或关闭报警规则集。
    -   报警规则集中包含多个报警规则，一个报警规则对应单个异常的检查项。多个报警规则集可以通过一个YAML资源配置到对应集群中，修改YAML会同步生成报警规则。
    -   关于报警规则YAML配置，请参见[如何通过CRD配置报警规则](#section_9ua_z95_ugc)。
    -   关于默认报警规则模板，请参见[默认报警规则模板](#section_mrd_x9o_nrr)。 |
    |**报警历史**|目前可查看最近发送的近100条历史记录。单击对应报警**报警规则类型**的链接，可跳转到对应监控系统中查看详细规则配置；单击对应报警**排查现场**的链接可快速定位到异常发生的资源页面（异常事件、指标异常的资源）。![报警历史查看](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5628476161/p255435.png) |
    |**联系人管理**|对联系人进行管理，可创建、编辑或删除联系人。|
    |**联系人分组管理**|对联系人分组进行管理，可创建、编辑或删除联系人分组。当无联系人分组时，控制台会从您的阿里云账号注册信息中同步创建一个默认联系人分组。|

5.  在报警规则管理页签，单击**编辑通知对象**可设置关联的通知对象；打开**启动状态**可开启对应报警规则集。


## 如何通过CRD配置报警规则

报警配置功能开启时，会默认在kube-system命名空间下创建一个AckAlertRule类型的资源配置，包含默认报警规则模板。容器服务报警规则集可通过此资源配置在集群中。

1.  登录[容器服务管理控制台](https://cs.console.aliyun.com)。

2.  在控制台左侧导航栏中，单击**集群**。

3.  在集群列表页面中，单击目标集群名称或者目标集群右侧**操作**列下的**详情**。

4.  在集群管理页左侧导航栏，选择**运维管理** \> **报警配置**。

5.  在报警规则管理页签中，单击右上角**编辑报警配置**，可查看当前集群中的AckAlertRule资源配置，并可通过YAML文件修改。

    报警规则配置的YAML文件示例如下：

    ```
    apiVersion: alert.alibabacloud.com/v1beta1
    kind: AckAlertRule
    metadata:
      name: default
    spec:
      groups:
        - name: pod-exceptions                     ## 报警规则集名。
          rules:
            - name: pod-oom                        ## 报警规则名。
              type: event                          ## 报警规则类型，枚举值为event（事件类型）、metric（指标类型）。
              expression: sls.app.ack.pod.oom      ## 报警规则表达式，当规则类型为event时，表达式的值为SLS日志服务事件报警sls_event_id。
              enable: enable                       ## 报警规则开启状态，枚举值为enable、disable。
            - name: pod-failed
              type: event
              expression: sls.app.ack.pod.failed
              enable: enable
    ```


## 默认报警规则模板

在以下情况下注册集群会默认创建相应报警规则：

-   开启默认报警规则功能。
-   未开启默认报警规则，首次进入报警规则页面。

默认创建的报警规则如下表所示。

|规则集类型|规则名|ACK\_CR\_Rule\_Name|SLS\_Event\_ID|
|-----|---|-------------------|--------------|
|critical-events集群重要事件报警规则集|集群error事件|error-event|sls.app.ack.error|
|集群warning事件|warn-event|sls.app.ack.warn|
|cluster-error集群异常事件报警规则集|集群节点docker进程异常|docker-hang|sls.app.ack.docker.hang|
|集群驱逐事件|eviction-event|sls.app.ack.eviction|
|集群GPU的XID错误事件|gpu-xid-error|sls.app.ack.gpu.xid\_error|
|集群节点重启|node-restart|sls.app.ack.node.restart|
|集群节点时间服务异常|node-ntp-down|sls.app.ack.ntp.down|
|集群节点PLEG异常|node-pleg-error|sls.app.ack.node.pleg\_error|
|集群节点进程异常|ps-hang|sls.app.ack.ps.hang|
|res-exceptions集群资源异常报警规则集|集群节点文件句柄过多|node-fd-pressure|sls.app.ack.node.fd\_pressure|
|集群节点磁盘空间不足|node-disk-pressure|sls.app.ack.node.disk\_pressure|
|集群节点进程数过多|node-pid-pressure|sls.app.ack.node.pid\_pressure|
|集群节点调度资源不足|node-res-insufficient|sls.app.ack.resource.insufficient|
|集群节点IP资源不足|node-ip-pressure|sls.app.ack.ip.not\_enough|
|pod-exceptions集群容器副本异常报警规则集|集群容器副本OOM|pod-oom|sls.app.ack.pod.oom|
|集群容器副本启动失败|pod-failed|sls.app.ack.pod.failed|
|集群镜像拉取失败事件|image-pull-back-off|sls.app.ack.image.pull\_back\_off|
|cluster-ops-err集群管控运维异常报警规则集|无可用LoadBalancer|slb-no-ava|sls.app.ack.ccm.no\_ava\_slb|
|同步LoadBalancer失败|slb-sync-err|sls.app.ack.ccm.sync\_slb\_failed|
|删除LoadBalancer失败|slb-del-err|sls.app.ack.ccm.del\_slb\_failed|
|删除节点失败|node-del-err|sls.app.ack.ccm.del\_node\_failed|
|添加节点失败|node-add-err|sls.app.ack.ccm.add\_node\_failed|
|创建VPC网络路由失败|route-create-err|sls.app.ack.ccm.create\_route\_failed|
|同步VPC网络路由失败|route-sync-err|sls.app.ack.ccm.sync\_route\_failed|
|安全巡检发现高危风险配置|si-c-a-risk|sls.app.ack.si.config\_audit\_high\_risk|
|托管节点池命令执行失败|nlc-run-cmd-err|sls.app.ack.nlc.run\_command\_fail|
|托管节点池未提供任务的具体命令|nlc-empty-cmd|sls.app.ack.nlc.empty\_task\_cmd|
|托管节点池出现未实现的任务模式|nlc-url-m-unimp|sls.app.ack.nlc.url\_mode\_unimpl|
|托管节点池发生未知的修复操作|nlc-opt-no-found|sls.app.ack.nlc.op\_not\_found|
|托管节点池销毁节点发生错误|nlc-des-node-err|sls.app.ack.nlc.destroy\_node\_fail|
|托管节点池节点排水失败|nlc-drain-node-err|sls.app.ack.nlc.drain\_node\_fail|
|托管节点池重启ECS未达到终态|nlc-restart-ecs-wait|sls.app.ack.nlc.restart\_ecs\_wait\_fail|
|托管节点池重启ECS失败|nlc-restart-ecs-err|sls.app.ack.nlc.restart\_ecs\_fail|
|托管节点池重置ECS失败|nlc-reset-ecs-err|sls.app.ack.nlc.reset\_ecs\_fail|
|托管节点池自愈任务失败|nlc-sel-repair-err|sls.app.ack.nlc.repair\_fail|
|cluster-network-err集群网络异常事件报警规则集|Terway资源无效|terway-invalid-res|sls.app.ack.terway.invalid\_resource|
|Terway分配IP失败|terway-alloc-ip-err|sls.app.ack.terway.alloc\_ip\_fail|
|解析ingress带宽配置失败|terway-parse-err|sls.app.ack.terway.parse\_fail|
|Terway分配网络资源失败|terway-alloc-res-err|sls.app.ack.terway.allocate\_failure|
|Terway回收网络资源失败|terway-dispose-err|sls.app.ack.terway.dispose\_failure|
|Terway虚拟模式变更|terway-virt-mod-err|sls.app.ack.terway.virtual\_mode\_change|
|Terway触发PodIP配置检查|terway-ip-check|sls.app.ack.terway.config\_check|
|Ingress重载配置失败|ingress-reload-err|sls.app.ack.ingress.err\_reload\_nginx|
|cluster-storage-err集群存储异常事件报警规则集|云盘容量少于20Gi限制|csi\_invalid\_size|sls.app.ack.csi.invalid\_disk\_size|
|容器数据卷暂不支持包年包月类型云盘|csi\_not\_portable|sls.app.ack.csi.disk\_not\_portable|
|挂载点正在被进程占用，卸载挂载点失败|csi\_device\_busy|sls.app.ack.csi.deivce\_busy|
|无可用云盘|csi\_no\_ava\_disk|sls.app.ack.csi.no\_ava\_disk|
|云盘IOHang|csi\_disk\_iohang|sls.app.ack.csi.disk\_iohang|
|磁盘绑定的PVC发生slowIO|csi\_latency\_high|sls.app.ack.csi.latency\_too\_high|
|磁盘容量超过水位阈值|disk\_space\_press|sls.app.ack.csi.no\_enough\_disk\_space|

